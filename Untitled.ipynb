{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Objective:\n",
    "The objective of this project is to categorify products based on their attibutes. \n",
    "\n",
    "The application of this project in business can be the automation of shelf arrangement services for retailer stores.  When a new product arrives, if we can correctly classify it based on the description, we can automatically know which shelf to put the item which increases the efficiency.\n",
    "\n",
    "Another application is for e-commerce. Categorifying products means grouping products that are potential substitutes. When customers search for one item, we can put the products in the same category in the 'Recommendation' section, which has the potential of increasing sales.\n",
    "\n",
    "### Project Details:\n",
    "The project has three steps\n",
    "\n",
    "1. Convert the UPCs to the product name and get different attributes associated with the product\n",
    "\n",
    "2. Build a machine learning model with Natual Language Processing techniques to classify the products based on their attributes\n",
    "\n",
    "3. Find the score of the brands of each product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are using the following packages for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/csun1992/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import matplotlib.image as img\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from retrying import retry\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "First load the UPCs to the notebook and get rid of all the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data scientist case study sample UPCs.csv'\n",
    "upcs = []\n",
    "with open(fileName) as file:\n",
    "    for i in file:\n",
    "        upc = json.loads(i.strip())\n",
    "        if upc != '':\n",
    "            upcs.append(str(int(upc)))\n",
    "upcs = list(set(upcs)) # get unique upcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Product Name for Each UPC\n",
    "Now we need to obtain the product names for each UPCs. \n",
    "\n",
    " The following issues are what I need to solve,\n",
    "1. Even though some online databases allow retrieving product name with UPC, they have restrictions on how many times queries can occur each day. Usually, the limit is 100 times.\n",
    "2. I used Walmart.com to find the attributes of products. However, the search engine of walmart.com is not ideal. Getting to the exact website of the product can be difficult.\n",
    "3. A large number of UPCs in the file do not correspond to any record in the databases. \n",
    "\n",
    "I cannot do too much for the third issue. The following methods are what I did not handle the first two issues.\n",
    "#### 1. I used the databases as a backup. The procedure of getting product names are as follows.\n",
    "- Use google search with the word 'UPC' and the actual UPC as the keywords.  \n",
    "- Scrape all the search results in the first search page. \n",
    "- Use MapReduce to find the first few words that occurred the most frequent in the search.  \n",
    "- Combine those words to the potential name of the product name.\n",
    "- Use databases if no google results or the name is not up to certain standard\n",
    "\n",
    "#### 2. To get to the exact Walmart product page, I used the following procedure since the Google search engine is more precise than Walmart search engine.\n",
    "- Google search with keyword consists of the word 'Walmart' and the product name retrieved.\n",
    "- Get to the first result that has a link to walmart.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get product name from upc\n",
    "def getProductName(upc): \n",
    "    wordRanks = defaultdict(int) # counts each word appears in search results\n",
    "    lines = 0\n",
    "    googleKeyWord = 'upc+'+upc \n",
    "    url = 'https://www.google.com/search?q=%s'%googleKeyWord # google link with keyword \"upc\" and upc number\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver')\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    for i in soup.find_all('h3', class_=\"LC20lb\"): # go through words in each result in the 1st page\n",
    "        bagOfWords = re.split('\\s*[,$%\\'\\-\\s|:;<>?\\(\\)\\[\\]\\/&]\\s*', i.text.lower())\n",
    "        for i in bagOfWords:\n",
    "            if i not in ['upc', 'gtin', 'ean', upc, '0'+upc, \\\n",
    "                        '00'+upc, '000'+upc, '0000'+upc, 'amazon', '...',\\\n",
    "                        'target', 'walmart', 'heb']: # not consider the words in the list\n",
    "                wordRanks[i] += 1\n",
    "        lines += 1\n",
    "    driver.quit()\n",
    "    if lines <= 1: # if no results or 1 result, use database to find the product name\n",
    "        try: \n",
    "            url = 'https://www.buycott.com/upc/%s'%upc # database 1\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            name = soup.find('div', {'id':'container_header'}).find('h2').text\n",
    "        except:\n",
    "            try:\n",
    "                url = 'https://www.upcitemdb.com/upc/%s'%upc # database 2\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                name = soup.find('p', class_='detailtitle').b.text.strip()\n",
    "            except:\n",
    "                raise Exception('no such item')\n",
    "    elif lines <= 5: # if less than 5 results, used words at least appear twice\n",
    "        name = [key for key, val in wordRanks.items() if val >= 2]\n",
    "        name = ' '.join(name)\n",
    "    else: \n",
    "        name = [key for key, val in wordRanks.items() if val >= lines * 0.4]\n",
    "        name = ' '.join(name)\n",
    "    if len(name.split(' ')) < 3: # if name is 3 words or less, use other databases\n",
    "        try: \n",
    "            url = 'https://www.buycott.com/upc/%s'%upc\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            name = soup.find('div', {'id':'container_header'}).find('h2').text\n",
    "        except:\n",
    "            try:\n",
    "                url = 'https://www.upcitemdb.com/upc/%s'%upc\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                name = soup.find('p', class_='detailtitle').b.text.strip()\n",
    "            except:\n",
    "                raise Exception('no such item')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walmartUrlFromGoogle(productName):\n",
    "    url = 'https://www.google.com/search?q=%s'%productName # google search with \"Walmart\" and product name\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver')\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    found = False\n",
    "    for i in soup.find_all('div', class_='r'): # find the first link that contains \"walmart\"\n",
    "        refTag = i.find('a')\n",
    "        if refTag:\n",
    "            link = refTag.get('href')\n",
    "            if link and link.split('.')[1].lower() == 'walmart':\n",
    "                found = True\n",
    "                break\n",
    "    driver.quit()\n",
    "    if found:\n",
    "        return link\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get product Walmart link from product name\n",
    "def getWalmartUrl(productName):\n",
    "    walmartName = '+'.join(re.split('\\s*[,\\-\\s]+\\s*', 'Walmart '+productName))\n",
    "    link = walmartUrlFromGoogle(walmartName)\n",
    "    if link:  # first try to get product name with google search\n",
    "        return link\n",
    "    url = 'https://www.walmart.com/search/?query=%s'%walmartName # if google search fails use walmart search\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver') \n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        link = soup.find('a', class_=\"product-title-link line-clamp line-clamp-2\").get('href')\n",
    "    except:\n",
    "        diver.quit()\n",
    "        raise Exception('no link')\n",
    "    driver.quit()\n",
    "    return 'https://www.walmart.com/%s'%link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadImages(imgUrl,title):\n",
    "    Picture_request = requests.get(imgUrl)\n",
    "    name = \"./images/%s.jpg\"%title\n",
    "    if Picture_request.status_code == 200:\n",
    "        with open(name, 'wb') as f:\n",
    "            f.write(Picture_request.content)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes of each result from the product Walmart link\n",
    "def getAttribute(productUrl, upc):\n",
    "    data=defaultdict()\n",
    "    page = requests.get(productUrl)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    try:\n",
    "        category = soup.find('ol', class_=\"breadcrumb-list\")\n",
    "        allSubCat = category.find_all('li')\n",
    "        cat = ['-'.join(re.split('\\s*[,&\\-\\s]+\\s*', i.find('a').get('itemname').lower())) for i in allSubCat]\n",
    "        data['category'] = '/'.join(cat)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        aboutProduct = soup.find('div', {'id':'about-product-section'})\n",
    "        try:\n",
    "            nutrition = aboutProduct.find('div', {'id':'nutritionFacts'})\n",
    "            infos = ['nutrition-facts-all-facts-servingSize', \"nutrition-facts-all-facts-calorie-info\",\\\n",
    "                    \"nutrition-facts-all-facts-nutrient-info\", \\\n",
    "                     \"nutrition-facts-all-facts-vitamins-minerals-info\"]\n",
    "            try:\n",
    "                for info in infos:\n",
    "                    fact = nutrition.find('div', class_=info).find_all('div')\n",
    "                    for i in fact:\n",
    "                        keyVal = [j.text for j in i.find_all('span')]\n",
    "                        if len(keyVal) >= 2:\n",
    "                            data[keyVal[0].lower()] = keyVal[1].lower()\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            specifications = aboutProduct.find('div', {'id':'specifications'}).find_all('tr')\n",
    "            for i in specifications:\n",
    "                keyVal = [j.text for j in i.find_all('td')]\n",
    "                if len(keyVal) >= 2:\n",
    "                        data[keyVal[0].lower()] = keyVal[1].lower()\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        ingredients = soup.find('span', class_=\"aboutModuleText\").text\n",
    "        if ingredients:\n",
    "            data['ingredients'] = ingredients.lower()\n",
    "        else:\n",
    "            aboutItem = soup.find('div', {'id':'product-about'})\n",
    "    except:\n",
    "        try:\n",
    "            aboutItem = soup.find('div', {'id':'product-about'}).find('ul').find_all('li')\n",
    "            for i in aboutItem:\n",
    "                line = i.text.split()\n",
    "                if line[-1] == 'serving' and line[-2] == 'per':\n",
    "                    data[line[-3].lower()] = line[0].lower()\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        reviews = soup.find('div', class_='ReviewHistogram ReviewsHeader-filter')\\\n",
    "                    .find_all('div', {'role':'button'})\n",
    "        reviews = [i.get('aria-label') for i in reviews]\n",
    "        reviewByStars = {}\n",
    "        for review in reviews:\n",
    "            splitedReview = re.split('[\\s\\-]', review)\n",
    "            value, key = int(splitedReview[0]), int(splitedReview[1]) \n",
    "            reviewByStars[key] = value\n",
    "        data['reviews'] = reviewByStars\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        imageLink = soup.find('div', class_='prod-hero-image').find('img').get('src')\n",
    "        imgName = downloadImages(imageLink, upc)\n",
    "        data['images'] = img.imread(imgName)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        singlePrice = soup.find('div', class_='variant-options-container')\\\n",
    "                        .find('div', class_=\"variant-ppu-price variant-option-text big bold\").text\n",
    "        if singlePrice != 'See price':\n",
    "            data['price'] = float(singlePrice[1:])\n",
    "        else:\n",
    "            allPrices = soup.find_all('div', class_='ppu-transactional-variant')[-1]\n",
    "            quant = int(allPrices.find('div', class_='LinesEllipsis').text.split()[0])\n",
    "            totalPrice = float(allPrices.find('div', class_='variant-ppu-price variant-option-text big bold')\\\n",
    "                             .text[1:])\n",
    "            data['price'] = totalPrice / quant\n",
    "    except:\n",
    "        try:\n",
    "            data['price'] = float(soup.find('span', class_='price-group').get('aria-label')[1:])\n",
    "        except:\n",
    "            pass\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the needed info about a product directly UPC\n",
    "def upcToProductInfo(upc):\n",
    "    try:\n",
    "        title = getProductName(upc)\n",
    "        try:\n",
    "            walmartUrl = getWalmartUrl(title)\n",
    "            try:\n",
    "                productAttr = getAttribute(walmartUrl, upc)\n",
    "                productAttr['name'] = title.lower()\n",
    "                return upc, productAttr\n",
    "            except:\n",
    "                raise Exception('something is wrong with attribute'+str(upc))\n",
    "        except:\n",
    "            raise Exception('google fault')\n",
    "    except:\n",
    "        raise Exception('no product name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop_max_attempt_number=5)\n",
    "def productInfoList(upcList):\n",
    "    productList = []\n",
    "    badQuery = []\n",
    "    for upc in upcList:\n",
    "        try:\n",
    "            productList.append(upcToProductInfo(upc))\n",
    "        except:\n",
    "            badQuery.append(upc)\n",
    "    return productList, badQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "data, badUpc = productInfoList(upcs) # get the data and bad UPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(productInfo, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data with categories as training set\n",
    "# Data without categories is used as test set\n",
    "allData.columns = ['_'.join(re.split('\\s*[\\-,&\\s\\|\\/]+\\s*', i)) for i in allData.columns]\n",
    "testData = allData[allData.category.isnull()].reset_index(drop=True)\n",
    "trainData = allData[allData.category.notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the unbalanced categories, I used SMOTE algorithm to upsample the minority classes.\n",
    "\n",
    "Due to extremely large number of food items, I first classify the item into categories 'food' and 'other'. For the \n",
    "items classified as 'food', I used hierarchical classification to further classify them into subcategories suggested by Walmart.\n",
    "\n",
    "Because there are features associated with only 'food' and non-food items. When doing the prediction, I first check whether the item has attributes associated with food or not. If yes, automatically perform second stage classification. Otherwise, first perform the first stage classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        clf1 = RandomForestClassifier(n_estimators=50)\n",
    "        clf2 = RandomForestClassifier(n_estimators=50)\n",
    "        parameters = {'max_depth':[2**i for i in range(2, 10)], \\\n",
    "                      'min_samples_split':[2**i for i in range(1, 8)], \\\n",
    "                      'max_features':['auto', 'sqrt', 'log2']}\n",
    "        self.clf1 = GridSearchCV(clf1, parameters, cv=5, scoring='f1')\n",
    "        self.clf2 = GridSearchCV(clf2, parameters, cv=5)\n",
    "\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=stopWords, ngram_range=(1, 2))\n",
    "\n",
    "        self.secondStageLabel = LabelEncoder()\n",
    "        \n",
    "    def transformTrainData(self, data):\n",
    "        cat1 = pd.Series([int(i.split('/')[0] == 'food') for i in data.category], name='cat1')\n",
    "        cat2Total = [i.split('/')[1] for i in data[cat1 == 1].category]\n",
    "        cat2Count = Counter(cat2Total)\n",
    "        cat2 = [i if cat2Count[i] >= 10 else 'other food' for i in cat2Total]\n",
    "        cat2 = self.secondStageLabel.fit_transform(cat2)\n",
    "        \n",
    "        trainData = data['name'].fillna('') + ', ' + data['ingredients'].fillna('')\n",
    "        try:\n",
    "            trainData = trainData + ', ' + data['form'].fillna('')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            trainData = trainData + ', ' + data['food_form'].fillna('')\n",
    "        except:\n",
    "            pass\n",
    "          \n",
    "        train2 = trainData[cat1 == 1]\n",
    "        return trainData, cat1, train2, cat2\n",
    "    \n",
    "    def transformTestData(self, data):\n",
    "        data = data.fillna('')\n",
    "        testData = data['name'] + ', ' + data['ingredients']\n",
    "        try:\n",
    "            testData = testData + ', ' + data['form']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            testData = testData + ', ' + data['food_form']\n",
    "        except:\n",
    "            pass\n",
    "        return self.vectorizer.transform([testData])\n",
    "    \n",
    "    def fit(self, data):\n",
    "        train1, label1, train2, label2 = self.transformTrainData(data)\n",
    "        train1 = self.vectorizer.fit_transform(train1)\n",
    "        train2 = self.vectorizer.transform(train2)\n",
    "  \n",
    "        smote = SMOTE('minority')\n",
    "        X1, y1 = smote.fit_sample(train1, label1)\n",
    "        X2, y2 = smote.fit_sample(train2, label2)\n",
    "        self.clf1.fit(X1, y1)\n",
    "        self.clf2.fit(X2, y2)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        label = []\n",
    "        foodFeatures = [\n",
    "       'calcium', 'calories', 'calories_from_fat', 'cholesterol',\n",
    "       'flavor', 'food_allergen_statements','food_form', 'ingredients', \n",
    "       'protein', 'saturated_fat', 'sugars', 'total_carbohydrate', \n",
    "        'total_fat', 'trans_fat', 'serving_per_container','serving_size'\n",
    "        ]\n",
    "        \n",
    "        nonFoodFeatures = ['animal_type', 'assembled_product_dimensions_(l_x_w_x_h)',\n",
    "       'assembled_product_weight', 'author', 'book_format', 'capacity', 'country_of_origin_assembly', \n",
    "       'cpu_socket_type', 'dietary_fiber', 'duration', 'fabric_content','isbn_10', 'isbn_13', \n",
    "        'makeup_form', 'manganese','manufacturer_part_number', 'manufacturer_part_number', 'material',\n",
    "       'maximum_ram_supported', 'model', 'number_of_pages', 'occasion', 'original_languages',\n",
    "       'publication_date', 'publisher', 'release_date', 'series_title', 'studio_production_company', \n",
    "       'tire_diameter', 'tire_load_index', 'tire_season','tire_size', 'tire_width',\n",
    "        'vehicle_make', 'vehicle_model']\n",
    "        \n",
    "        foodFeatIncluded = [i for i in foodFeatures if i in data.columns]\n",
    "        nonFoodFeatIncluded = [i for i in nonFoodFeatures if i in data.columns]\n",
    "        for _, row in data.iterrows():\n",
    "            # preliminary check whether the product is food or not based on whehter it has\n",
    "            # attibutes associated with food or non-food\n",
    "            if row[foodFeatIncluded].notnull().any():  \n",
    "                row = self.transformTestData(row)\n",
    "                result = self.clf2.predict(row)\n",
    "                label.extend(self.secondStageLabel.inverse_transform(result))\n",
    "            elif row[nonFoodFeatIncluded].notnull().any():\n",
    "                result.append('other')\n",
    "            else:\n",
    "                row = self.transformTestData(row)\n",
    "                if self.clf1.predict(row)[0]:\n",
    "                    result = self.clf2.predict(row)\n",
    "                    label.extend(self.secondStageLabel.inverse_transform(result))\n",
    "                else:\n",
    "                    label.append('other')\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csun1992/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "clf.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fresh-food',\n",
       " 'snacks-cookies-chips',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'condiments-sauces-spices',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'snacks-cookies-chips',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'other food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'condiments-sauces-spices',\n",
       " 'other',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'condiments-sauces-spices',\n",
       " 'condiments-sauces-spices',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'condiments-sauces-spices',\n",
       " 'coffee',\n",
       " 'snacks-cookies-chips',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'other food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'snacks-cookies-chips',\n",
       " 'condiments-sauces-spices',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'baking',\n",
       " 'condiments-sauces-spices',\n",
       " 'other',\n",
       " 'coffee',\n",
       " 'baking',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'baking',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'other food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'other food',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'snacks-cookies-chips',\n",
       " 'other food',\n",
       " 'fresh-food',\n",
       " 'other',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'beverages',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'snacks-cookies-chips',\n",
       " 'other food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'baking',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'condiments-sauces-spices',\n",
       " 'beverages',\n",
       " 'condiments-sauces-spices',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'baking',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'other food',\n",
       " 'fresh-food',\n",
       " 'snacks-cookies-chips',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'condiments-sauces-spices',\n",
       " 'beverages',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'coffee',\n",
       " 'coffee',\n",
       " 'condiments-sauces-spices',\n",
       " 'fresh-food',\n",
       " 'snacks-cookies-chips',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'fresh-food',\n",
       " 'meal-solutions-grains-pasta',\n",
       " 'snacks-cookies-chips',\n",
       " 'beverages']"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(testData) # result for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand score is obtained based on the Walmart review\n",
    "The score is average star of each product associated with same brand by dropping product with less than 5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = list(set(df.brand.dropna()))\n",
    "for i in df[df.brand.isnull()].index:\n",
    "    for brand in brands:\n",
    "        if re.match(brand.lower(), df.loc[i, 'name'].lower()) \\\n",
    "        or re.match(' '.join(re.split('\\s*-\\s*', brand.lower())), df.loc[i, 'name'].lower()):\n",
    "            df.loc[i, 'brand'] = brand\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBrandScores(df):\n",
    "    scores = defaultdict(int)\n",
    "    for name, group in df.groupby('brand'):\n",
    "        totalProduct = 0\n",
    "        for review in group.reviews.dropna():\n",
    "            if sum(review.values()) <= 5:\n",
    "                pass\n",
    "            else:\n",
    "                scores[name] += sum([i * j for i, j in zip(review.keys(), review.values())])\n",
    "                totalProduct += 1\n",
    "        if totalProduct > 0:\n",
    "            scores[name] = scores[name] / totalProduct\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brand scores are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'5strands': 35.0,\n",
       "             'a.1.': 238.7,\n",
       "             'athenos': 69.75,\n",
       "             'bagel bites': 92.0,\n",
       "             \"baker's\": 90.0,\n",
       "             'boca': 265.57142857142856,\n",
       "             \"breakstone's\": 349.5,\n",
       "             \"bull's-eye\": 151.0,\n",
       "             'bumble bee': 380.0,\n",
       "             'calumet': 33.0,\n",
       "             'capri sun': 216.125,\n",
       "             'certo': 130.0,\n",
       "             'cheez whiz': 201.0,\n",
       "             'classico': 190.0,\n",
       "             'claussen': 192.75,\n",
       "             'community coffee': 458.0,\n",
       "             'cool whip': 30.0,\n",
       "             'corn nuts': 52.0,\n",
       "             'cornnuts': 40.4,\n",
       "             'country time': 288.0,\n",
       "             'cracker barrel cheese': 242.0,\n",
       "             'crystal light': 252.6315789473684,\n",
       "             'delimex': 67.0,\n",
       "             'devour': 25.0,\n",
       "             'dream whip': 140.0,\n",
       "             'franks': 27.0,\n",
       "             \"french's\": 198.0,\n",
       "             'gevalia': 1883.888888888889,\n",
       "             'good seasons': 186.0,\n",
       "             'great value': 191.0,\n",
       "             'great value organic': 65.0,\n",
       "             'grey poupon': 425.0,\n",
       "             'healthy choice': 969.0,\n",
       "             'heinz': 1479.5714285714287,\n",
       "             \"hershey's\": 1349.0,\n",
       "             'hillshire farm': 52.0,\n",
       "             \"jack daniel's\": 147.33333333333334,\n",
       "             'jell-o': 105.375,\n",
       "             'jet-puffed': 162.45,\n",
       "             'just crack an egg': 79.0,\n",
       "             'knox': 71.0,\n",
       "             'knudsen': 26.0,\n",
       "             'kool-aid': 234.3846153846154,\n",
       "             'kraft': 362.58762886597935,\n",
       "             'lance': 382.0,\n",
       "             'lea & perrins': 435.0,\n",
       "             'lean cuisine': 406.0,\n",
       "             'lunchables': 791.0,\n",
       "             'maxwell house': 368.1388888888889,\n",
       "             'maxwell house, maxwell house international': 638.0,\n",
       "             'mayo': 164.0,\n",
       "             'mccafe': 467.25,\n",
       "             'mcp': 21.0,\n",
       "             'mio': 118.66666666666667,\n",
       "             'miracle whip': 130.0,\n",
       "             'mrs. grass': 70.0,\n",
       "             \"nancy's\": 119.0,\n",
       "             'ore-ida': 191.75,\n",
       "             'oscar mayer': 265.82758620689657,\n",
       "             'osteo bi-flex': 136.0,\n",
       "             'oven fry': 149.0,\n",
       "             'philadelphia': 290.2,\n",
       "             'planters': 303.5,\n",
       "             'rust-oleum': 33.0,\n",
       "             \"sam's choice\": 280.0,\n",
       "             'sanka': 115.0,\n",
       "             'shake n bake': 175.33333333333334,\n",
       "             'smart ones': 51.666666666666664,\n",
       "             'starkist': 98.0,\n",
       "             'stove top': 301.6666666666667,\n",
       "             'sure-jell': 188.0,\n",
       "             'taco bell': 217.5,\n",
       "             'tang': 349.25,\n",
       "             'tassimo': 810.0,\n",
       "             'tgi fridays': 99.0,\n",
       "             'tostitos': 250.0,\n",
       "             'velveeta': 272.6,\n",
       "             'vermont maid': 39.0,\n",
       "             \"wyler's\": 95.0,\n",
       "             'yuban': 168.33333333333334})"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(t[1] for t in data)\n",
    "df.index = [i[0] for i in data]\n",
    "df = df.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n",
    "getBrandScores(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
